/* 
  Anelosimus.Eximius.pgc : This process gets a list of URLs from 
  the table "node", downloads them, inserts the new URLs into table node,
  and new links into table "links".

    Copyright (C) 2011  Pierre Jourlin

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
 
    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.
***********************************************************************************
  Anelosimus.Eximius.pgc : Ce processus récupère une liste d'URLs dans
  la table "node", les télécharge, insère dans la même table les nouvelles
  URLs découvertes et dans la table "links", les nouveaux liens.

  Copyright (C) 2011 Pierre Jourlin — Tous droits réservés.
 
  Ce programme est un logiciel libre ; vous pouvez le redistribuer ou le
  modifier suivant les termes de la “GNU General Public License” telle que
  publiée par la Free Software Foundation : soit la version 3 de cette
  licence, soit (à votre gré) toute version ultérieure.
  
  Ce programme est distribué dans l’espoir qu’il vous sera utile, mais SANS
  AUCUNE GARANTIE : sans même la garantie implicite de COMMERCIALISABILITÉ
  ni d’ADÉQUATION À UN OBJECTIF PARTICULIER. Consultez la Licence Générale
  Publique GNU pour plus de détails.
  
  Vous devriez avoir reçu une copie de la Licence Générale Publique GNU avec
  ce programme ; si ce n’est pas le cas, consultez :
  <http://www.gnu.org/licenses/>.

    Pierre Jourlin
    L.I.A. / C.E.R.I.
    339, chemin des Meinajariès
    BP 1228 Agroparc
    84911 AVIGNON CEDEX 9
    France 
    pierre.jourlin@univ-avignon.fr
    Tel : +33 4 90 84 35 32
    Fax : +33 4 90 84 35 01

*/

#include <stdlib.h>
#include <stdio.h>
#ifndef WIN32
#include <unistd.h>
#endif
#include <curl/multi.h>
#include <sys/types.h> 
#include <signal.h>

#define MAXURLSIZE	(100*1024) /* 100 Kb */
#define MAXCO		10000	// Max parrallel connections
#define MAXTLD		10	// Max number of chars in top level domain

exec sql include sqlca;
exec sql begin declare section;
	struct MemoryStruct {
		char *memory;
		unsigned long int size;
	};
exec sql end declare section;
exec sql begin declare section;
	unsigned long int MaxPageSize=0;
	int nurls;
	struct MemoryStruct currentBody[MAXCO];
	char currentURL[MAXURLSIZE];
	char url_chunk[MAXCO][MAXURLSIZE];
	short int url_todo[MAXCO];
	char currentTLD[MAXURLSIZE];
	char currentRoot[MAXURLSIZE];
	char currentRootURL[MAXURLSIZE];
	unsigned long int currentID;
	unsigned long int toID;
	char currentEffectiveRootURL[MAXURLSIZE];
exec sql end declare section;
EXEC SQL BEGIN DECLARE SECTION;
const char *target = "webcrawler@localhost"; // Data for connection if needed
const char *user = "jourlin";
const char *password = "??????????";
EXEC SQL END DECLARE SECTION;

void terminate(int sig);

void checkErrorCode(void){
	if(sqlca.sqlcode!=0){
		if(sqlca.sqlcode!=-403){
    			printf("error code %ld, message %s, rows %ld, warning %c\n", 
    			sqlca.sqlcode,sqlca.sqlerrm.sqlerrmc, sqlca.sqlerrd[2], 
    			sqlca.sqlwarn[0]);
		}
		else {
			// not necessarily wrong : might already got this url
		}
	}
}


size_t curl_write( void *ptr, size_t size, size_t nmemb, void *userdata)
{	
	struct MemoryStruct *mem= &currentBody[(long int) userdata];
	mem->memory=realloc(mem->memory, ((size_t) mem->size+1) + size*nmemb); // extend memory
	
	if(mem->memory==NULL){
   		 /* out of memory! */ 
    		printf("not enough memory (realloc returned NULL)\n");
    		exit(EXIT_FAILURE);
  	}
	memcpy(&(mem->memory[mem->size]), ptr, size*nmemb);
	mem->size+=(unsigned long int) (size*nmemb);
	mem->memory[mem->size] = 0;
	return size*nmemb;
}

int getNextURL(bool reinit, int idx){
	static const char *tag="href=\"";
	static char *start;

	if(reinit){
		start=currentBody[idx].memory;
		return TRUE;
	}
	char *end=start;
	start=strstr(start, tag);
	if(start!=NULL){
		start+=strlen(tag);
		end=strstr(start, "\"");
		if(end!=NULL){
			if((end-start) <MAXURLSIZE){
				memcpy(currentURL, start, (size_t) (end-start));
				*(currentURL+(size_t) (end-start))='\0';
			}
			else
				end=NULL;
		}
		start=end;
	}
	return (start!=NULL);
}

void GetTLD(char* tld, char* url)
{
	char  *tmp;
	while(*url!='\0' && *url !='/') { // end of the protocol's field
		url++;
	}	
	url++;
	if(*url!='/')
		*tld='\0';	// something wrong
	else 
		url++;
	while(*url!='\0' && *url !='/') { // end of the protocol's field
		url++;
	}	
	// *url should now contain the next character after the last character of tld
	tmp=url-1;
	while(*tmp!='.')
		tmp--; // find the 1st char of TLD; 
	tmp++;
	if(url-tmp < MAXTLD)
		while(tmp!=url)
			*tld++=*tmp++; // copy tld
	*tld='\0';
	
}
int insertURL(void){
	static char tmp[MAXURLSIZE];	 
	char *offset;

	if(currentURL[0]=='/'){				// deals with relative paths
		strcpy(tmp, currentURL);
		strcpy(currentURL, currentRoot);
		strcat(currentURL, tmp);
	};	
	if(currentURL[strlen(currentURL)-1]=='/')
		currentURL[strlen(currentURL)-1]='\0'; // remove final '/'
	if(strstr(currentURL, "://")==NULL) // ignore badly formed urls
		return 0;	
	GetTLD(currentTLD, currentURL);
	exec sql BEGIN WORK;
	exec sql INSERT INTO node (url, checked, tld) VALUES (:currentURL, NULL, :currentTLD) RETURNING id INTO :toID; 
	checkErrorCode();
	exec sql COMMIT WORK;
	exec sql BEGIN WORK;
        exec sql INSERT INTO links ("from", "to") VALUES (:currentID , :toID) ; 
	checkErrorCode();
        exec sql COMMIT WORK;
}

void getDirFromURL(char *url){
	char *offset=url+strlen(url)-1;
	while(offset >= url && *offset!='/')
		offset--;
	if(offset>=url+3 && *offset=='/' && *(offset-1)=='/' && *(offset-2)==':'){
		// found nothing to trim
		return;	
	}
	else{
		*offset='\0';
	}
}

static void init(CURLM *cm, int i, unsigned long header)
{
  CURL *eh = curl_easy_init();
  
  curl_easy_setopt(eh, CURLOPT_URL, url_chunk[i]);
  curl_easy_setopt(eh, CURLOPT_FOLLOWLOCATION, 1);
  curl_easy_setopt(eh, CURLOPT_WRITEFUNCTION, curl_write);
  curl_easy_setopt(eh, CURLOPT_WRITEDATA, (void *) (long int) i);
  curl_easy_setopt(eh, CURLOPT_HEADER, header);
  curl_easy_setopt(eh, CURLOPT_NOBODY, header);
  curl_easy_setopt(eh, CURLOPT_TIMEOUT, 1L); 
  curl_easy_setopt(eh, CURLOPT_PRIVATE, url_chunk[i]);
  curl_easy_setopt(eh, CURLOPT_VERBOSE, 0L);
 
  curl_multi_add_handle(cm, eh);
}



void terminate(int sig) {
	exec sql UPDATE node SET checked=NULL, effectiveurl=NULL WHERE url=:currentRootURL;
	exec sql disconnect all;
        printf("\nInterrupted ! Cancelling the crawl on %s\n", currentRootURL);
	curl_global_cleanup();
        exit(sig);
}

void xhtml2text(char *mem){
	char *pt=mem;
	while(*pt!=0){				// parse until the end of string
		while(*pt!=0 && *pt!='<')	// copy text when outside a tag
			*mem++=*pt++;
		while(*pt!=0 && *pt!='>')	// ignore text when inside a tag
			pt++;
		if(*pt){
			*mem++='\n';
			pt++;
		}
	};
	*mem=0;					// terminate the new, shortened string
}

void drop_blanks(char *mem){
	char *pt=mem;
	while(*pt!=0){				// parse until the end of string
		while(*pt!=0 && *pt!=' ' && *pt!='\t' && *pt !='\r' && *pt!='\n')	// copy text
			*mem++=*pt++;
		while(*pt!=0 && (*pt==' '||*pt=='\t'||*pt=='\r'||*pt=='\n'))	// ignore blanks
			pt++;
		if(*pt)
			*mem++=' ';
	};
	*mem=0;					// terminate the new, shortened string
}

void drop_content(char *mem, const char *tstart, const char *tend){
	char *start, *end;

	start=strstr(mem, tstart);
	do{
		if(start==NULL)	// Done
			return;
		else
			end=strstr(start, tend);
		if(end==NULL)
			*start=0;	// remove all the text starting with <script 
		else
			strcpy(start, end+strlen(tend));	// remove the text between script tags
		start=strstr(start, tstart);
	}while(1);
}

int main(void) {
	CURL *curl;
	CURLcode res;

	CURLM *cm;
  	CURLMsg *msg;
  	long nburls, L;
  	unsigned int C=0;
  	int M, Q, U;
  	fd_set R, W, E;
  	struct timeval T;

	pid_t pid;
	if ((pid = getpid()) < 0) {
	  perror("unable to get pid");
	};
	

	(void) signal(SIGINT,terminate);

// 	uncomment the following line if you need database details for the connection
	exec sql connect to :target USER :user USING :password;
// 	if no details are needed, simply do :
//	exec sql connect to 'unix:postgresql:locahost' ;
	checkErrorCode();
	exec sql SET CLIENT_ENCODING TO 'LATIN1'; 
	while(1){
		M=Q=U=-1; // re-init error flags
		EXEC SQL BEGIN WORK;
		exec sql PREPARE get_url FROM "SELECT url, id FROM node WHERE (tld='fr' OR tld='org' OR tld='com') AND checked IS NULL ORDER BY length(url) ASC LIMIT 10000 FOR UPDATE ;";
		checkErrorCode();
		EXEC SQL DECLARE url_cursor CURSOR FOR get_url;
		checkErrorCode();
		/* when end of result set reached, break out of while loop */
		EXEC SQL WHENEVER NOT FOUND DO BREAK;
		checkErrorCode();
		EXEC SQL OPEN url_cursor;
		checkErrorCode();
		nburls=0;
		while(1){
			EXEC SQL FETCH NEXT FROM url_cursor INTO :currentRootURL, :currentID;
			checkErrorCode();
			if(sqlca.sqlcode!=0)
				return;
			exec sql UPDATE node SET checked=now() WHERE url=:currentRootURL;
	                checkErrorCode();
	                if(sqlca.sqlcode!=0)
	                        return;
			strcpy(url_chunk[nburls], currentRootURL);
			printf("[%d] is processing %s\n", pid, url_chunk[nburls]);
			nburls++;
		}
		EXEC SQL CLOSE url_cursor;
		EXEC SQL COMMIT WORK;
		curl_global_init(CURL_GLOBAL_ALL);
		cm = curl_multi_init();
		for (C = 0; C < nburls; ++C) {
    			init(cm, C, 1);			/* Only headers at the moment */
			getNextURL(TRUE, C); 		// initialize
			currentBody[C].memory=malloc(1); // For the null char
			currentBody[C].size=0;	// no char at this point  

  		}
		nburls=0; // reused for counting html and xml urls
		while (U) {
			curl_multi_perform(cm, &U);
	 		if (U) {
 				FD_ZERO(&R);
				FD_ZERO(&W);
				FD_ZERO(&E);
 
				if (curl_multi_fdset(cm, &R, &W, &E, &M)) {
					fprintf(stderr, "E: curl_multi_fdset\n");
					return EXIT_FAILURE;
				}
 
				if (curl_multi_timeout(cm, &L)) {
					fprintf(stderr, "E: curl_multi_timeout\n");
					return EXIT_FAILURE;
				}
				if (L == -1)
					L = 100;
	 
				if (M == -1) {
					#ifdef WIN32
					Sleep(L);
					#else
					sleep(L / 1000);
					#endif
				} else {
					T.tv_sec = L/1000;
					T.tv_usec = (L%1000)*1000;
 					if (0 > select(M+1, &R, &W, &E, &T)) {
						fprintf(stderr, "E: select(%i,,,,%li): %i: %s\n", M+1, L, errno, strerror(errno));
        	  				return EXIT_FAILURE;
        				}
      				}
			}
 			while ((msg = curl_multi_info_read(cm, &Q))) {
				if (msg->msg == CURLMSG_DONE) {
					char *url, *eurl, *ct;
					CURL *e = msg->easy_handle;
					curl_easy_getinfo(msg->easy_handle, CURLINFO_PRIVATE, &url);
					curl_easy_getinfo(msg->easy_handle, CURLINFO_EFFECTIVE_URL, &eurl);
					curl_easy_getinfo(msg->easy_handle, CURLINFO_CONTENT_TYPE, &ct);
					if(ct && (strstr(ct, "text/html")!=NULL || strstr(ct, "text/xml")!=NULL)){
						strcpy(url_chunk[nburls], eurl);
					}

					curl_multi_remove_handle(cm, e);
					curl_easy_cleanup(e);
				}
				else {
        				fprintf(stderr, "E: CURLMsg (%d)\n", msg->msg);
				}
				nburls++;
			}
		}
		for (C = 0; C < nburls; ++C) 
			if(currentBody[C].memory){
				free(currentBody[C].memory);
			}

		/***   Get links ****/
		M=Q=U=-1; // re-init error flags
		for (C = 0; C < nburls; ++C) {
    			init(cm, C, 0);			/* Now get Full pages */
			getNextURL(TRUE, C); // initialize
			currentBody[C].memory=malloc(1); // for the null char
			currentBody[C].size=0;	// no data at this point, only the terminal null char  
  		}

		while (U) {
			curl_multi_perform(cm, &U);
	 		if (U) {
 				FD_ZERO(&R);
				FD_ZERO(&W);
				FD_ZERO(&E);
 
				if (curl_multi_fdset(cm, &R, &W, &E, &M)) {
					fprintf(stderr, "E: curl_multi_fdset\n");
					return EXIT_FAILURE;
				}
 
				if (curl_multi_timeout(cm, &L)) {
					fprintf(stderr, "E: curl_multi_timeout\n");
					return EXIT_FAILURE;
				}
				if (L == -1)
					L = 100;
	 
				if (M == -1) {
					#ifdef WIN32
					Sleep(L);
					#else
					sleep(L / 1000);
					#endif
				} else {
					T.tv_sec = L/1000;
					T.tv_usec = (L%1000)*1000;
 					if (0 > select(M+1, &R, &W, &E, &T)) {
						fprintf(stderr, "E: select(%i,,,,%li): %i: %s\n", M+1, L, errno, strerror(errno));
        	  				return EXIT_FAILURE;
        				}
      				}
			}

 			while ((msg = curl_multi_info_read(cm, &Q))) {
				if (msg->msg == CURLMSG_DONE) {
					char *url;
					CURL *e = msg->easy_handle;
					curl_easy_getinfo(msg->easy_handle, CURLINFO_PRIVATE, &url);
					//fprintf(stderr, "R: %d - %s <%s>\n", msg->data.result, curl_easy_strerror(msg->data.result), url);
					curl_multi_remove_handle(cm, e);
					curl_easy_cleanup(e);
				}
				else {
        				fprintf(stderr, "E: CURLMsg (%d)\n", msg->msg);
				}
			}
		}
		
		for (C = 0; C < nburls; ++C) {
			getNextURL(TRUE, C);
			strcpy(currentRootURL, url_chunk[C]);
  	 		while(getNextURL(FALSE, C)){
				insertURL();
			}
			if(currentBody[C].size > MaxPageSize){
				MaxPageSize=currentBody[C].size;
				fprintf(stderr, "*** New maximum page size %ld\n", MaxPageSize);
			}
			drop_content(currentBody[C].memory, "<!--", "-->");

			drop_content(currentBody[C].memory, "<script ", "</script>");
			drop_content(currentBody[C].memory, "<style ", "</style>");
			xhtml2text(currentBody[C].memory);
			drop_blanks(currentBody[C].memory);
			exec sql UPDATE node SET content=:currentBody[C].memory WHERE url=:url_chunk[C];
			//printf("Content of %s:\n [%s]\n", url_chunk[C], currentBody[C].memory);
			if(currentBody[C].memory)
				free(currentBody[C].memory);
  		}
		curl_multi_cleanup(cm);
		curl_global_cleanup();
		
	}
	EXEC SQL disconnect all;
	return EXIT_SUCCESS;
}

